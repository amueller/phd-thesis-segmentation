future / present tense?!?!?
include more images
most important parts: intros and conclusions!
Give detailed results for MSRC and Pascal
Visualize Weights wherever possible!
bibliography consistency! (first names?)
Datasets? Ganz an den Anfang hinter die introduction?
Superpixels!!?

STRUKTUR: zuerste conditioning wichtig oder zuerst exact lernen? NYU?
Erstmal modelle, auswerten, ... dann am schluss: so haben wir das exakt gemacht.
oder erst exakt, dann "gut mal, das ist wichtig bei den modellen", dann nyu?

SQUARED AVERAGING!

s/learning rate/step size/g  Done (check again later)

1-slack und n-slack consistent (one/1, n, $n$)  (done, check again later)

Itm
=====
* clustering: bag of word experiments? Segmentation experiments? mnist experiments?  ONLY MNIST
* remove MLPack runtime claims from ITM? DONE
* redo summary DONE
* ALGORITHM FIX!! DONE
* beschreibe benutzung der dim estimation DONE
* optional: table dimension estimates

weak supervision
==================
multi-class...
kick out?
korrekturlesen?

intro to structured prediction
============================================
Consistency \phi \Phi DONE
subgradient learning rate citations?
Table complexity comparisons?
Graphiken?

Pystruct
========
wann fuere ich die abkuerzung SSVM ein? DONE
mehr ueber modelle? conditional? -> meh erstmal nicht?
another row in library table: bundle methods BMRM ?
ALGORITM table update, dlib algorithms?
Pairwise?! -> meh erstmal nicht ^^

exact learning
=================
referenzen auf algorithm davor
viele todos!!! schlecht geschrieben :-/ 
multi-label
snakes?
snakes / multi-label bei caching!? caching koennen wir sogar auf sequence
ausprobieren!

TO WRITE: comparison of learning algs for large-scale
========================================================
Dataset? Applications?
Segmentation? Chains ocr / colln?


TO WRITE: Importance of conditioning in edges!
===============================================

TO WRITE: latent variable stuff?
=================================
