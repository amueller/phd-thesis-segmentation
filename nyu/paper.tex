
\chapter{Learning Depth-Sensitive Conditional Random Fields}\label{ch:nyu}

%\section{Introduction}
For robots to perform varied tasks in unstructured environments, understanding their
surroundings is essential. In this chapter, we look at the semantic annotation of maps
as a dense labeling of \mbox{RGB-D} images into semantic classes. We formulate the problem
as learning a CRF over a superpixel segmentation of the \mbox{RGB-D} image, producing a labeling
that takes 3D layout into account. Dense labeling of objects
and structure classes allows for a detailed reasoning about the scene.

%We use of random forests to obtain a noisy low-level estimate of structure class on pixel level.
%These estimates serve as the unary input to the learned CRF model which integrates
%these low-level cues to a consistent interpretation of the scene.

We thereby extend the success of learned CRF models for semantic segmentation
in RGB images as considered in Chapter~\ref{ch:exact_learning} to the domain of 3D scenes.
Our emphasis lies on exploiting the additional depth and 3D information in all
processing steps, while relying on \emph{learning} to create a model that 
is adjusted to the properties of the sensor input and environment.

Our approach starts with a random forest, providing a noisy local estimate
of semantic classes based on color and depth information. These estimates
are grouped together using a superpixel approach, for which we extend previous
superpixel algorithms from the RGB to the \mbox{RGB-D} domain.
We then build a geometric model of the scene, based on the neighborhood graph
of superpixels.  We use this graph not only to capture spatial relations in the
2D plane of the image, but also to model object distances and surface angles in
3D, using a point cloud generated from the \mbox{RGB-D} image. The process is depicted
in \Figref{teaser}.

We assess the accuracy of our model on the challenging NYU Segmentation V2
dataset \citep{SilbermanECCV12}, where our model outperforms previous
approaches.  Our analysis shows that while our random forest model
already has competitive performance, the superpixel-based grouping and in
particular the loss-based learning are integral ingredients of the success of our method.

\begin{figure}
    \begin{center}
        \includegraphics[width=\linewidth]{nyu/images/teaser}
    \end{center}
    \caption{%
        Overview of the proposed semantic segmentation method.\figlabel{teaser}
    \figlabel{height}}
\end{figure}


\section{Related Work}
The task of dense semantic annotation of 3D maps has seen an increased interest
in recent years.  Early work includes \citet{nuchter2008towards}, who combined
6D SLAM, surface annotation, and object recognition to build semantically
annotated maps. They demonstrate their approach on a mobile robot in an indoor
environment.
%
More recently \citet{sengupta2012automatic} introduced a dataset of
semantically annotated street-scenes in a closed track, captured as pairs of
stereo images. They approach the task by jointly reasoning about 3D layout and
semantics of the scenes and produce a dense labeling on image level.
\citet{senguptaurban} extended the approach to produce a volumetric
reconstruction of the scene, together with a dense semantic labeling of the
volumetric representation. Their approach to image segmentation builds on
the hierarchical CRF approach of
\citet{ladicky2009associative}, which is similar in spirit to our approach,
but used Potts potentials together with cross-validation to adjust parameters.

Recent approaches for indoor semantic annotation of maps mostly focused on
\mbox{RGB-D} images, which are now easy to obtain using structured light sensors.
\citet{stuckler2012semantic}, for example, used a Random Forest model to obtain a dense
semantic labeling of images and integrated predictions over multiple views in 3D.
They evaluated their approach on table-top and simple indoors scenes.
%
\citet{silberman2011indoor} introduced the NYU Depth Dataset V1 indoor dataset,
which consisted of a large variety of densely annotated indoor scenes, captured as \mbox{RGB-D} images.
Their work also introduced a baseline method for semantic segmentation of \mbox{RGB-D}
image, which is based on a CRF over superpixels, with unary potentials given by interest point descriptors.
While pairwise potentials for the CRF were carefully
designed for the dataset, potentials were either directly set by hand or estimated using empirical
frequencies. This is in contrast to our approach which applies structured prediction techniques
to learn potentials automatically to optimize predictive performance.
%
\citet{ren2012rgb} evaluated the design of features for semantic labeling of
\mbox{RGB-D} data, and use a hierarchical segmentation to provide context. While they
also define a CRF on superpixels, their model is again not learned, but a
weighted Potts model, using only a probability of boundary map, and not taking
spatial layout into account at all.
%
\citet{SilbermanECCV12} extended the NYU Depth Dataset V1 to the NYU Depth Dataset V2 that we are also
using here.  Their focus is on inferring support relations in indoor
scenes, such as objects resting on tables or shelfs, which in turn rest on the floor.
Their approach is based on robust estimation of 3D plane hypotheses, which are then jointly
optimized with support relations and structure classes.
\citet{SilbermanECCV12} used a complex pipeline, employing significant domain knowledge.
In our approach, on the other hand, we try to learn all relevant domain specific features
directly from the data, which allows us to out-perform the work of \citet{SilbermanECCV12}
with respect to structure class segmentation.

\citet{couprie-iclr-13} approached the task of semantic segmentation of structure classes
in \mbox{RGB-D} using the paradigm of convolutional neural networks, extending
previous work of \citet{farabet-pami-13} and \citet{schulz2012learning}.
Similar to our approach, \citet{couprie-iclr-13} combined the output of a
pixel-based, low-level learning algorithm with an independent unsupervised
segmentation step. In contrast to their work, we improve our results by not
only averaging predictions within superpixels, but also explicitly learning
interactions between neighboring superpixels, favoring a consistent
interpretation of the whole image.

\citet{stueckler2013} extended the approach of \citet{stuckler2012semantic} to
a real-time system for online learning and prediction of semantic classes.
Their method use a GPU implementation of random forests, and integrate 3D scene
information in an online fashion. They evaluated their approach on the dataset
of \citet{SilbermanECCV12} with promising results.  We use the implementation
of random forests provided by \citet{stueckler2013}, but instead of integrating
predictions over time, we focus on exploiting the structure within a single
frame.

While many of the works mentioned in this chapter make use of a CRF
approach, we are not aware of any prior work on semantic annotation of 3D maps
that fully learns their potentials.
%The task of dense semantic labeling in natural images has been widely studied
%in computer vision.  Our approach of using pixel-based low-level cues is
%similar to the work of \citet{shotton2006textonboost}, who introduced
%TextonBoost for semantic segmentation, and used a hand-coded CRF energy to
%model spatial interactions. More recent work usually employ learned conditional
%random fields, learning object interactions from the
%data to directly maximize a loss function~\citep{lucchi2013learning, krahenbuhl2012efficient},
%which is the approach we are following.


\section[Learning Depth-Sensitive Conditional Random Fields]{Learning Depth-Sensitive\linebreak Conditional Random Fields}\seclabel{learning}
We take a CRF approach, whose nodes represent a labeling of
superpixels.  We use an energy consisting of first and second order factors
(also called unary and pairwise potentials), with learned potential functions.
Let us denote the representation of an input image by $x$ and a labeling of
superpixels into semantic classes by $y$.  Then the general form of the energy is
\begin{equation}\eqlabel{energy}
    g(x, y) = \sum_{v \in V} \psi_v(x, y_v) + \sum_{(v, w) \in E} \psi_{v,w}(x, y_{v}, y_{w}).
\end{equation}
Here $V$ enumerates the superpixels, and $E\subset V \times V$ is a set of
edges, encoding adjacence between superpixels.

We learn the unary and pairwise energy functions $\psi_v$ and $\psi_{v, w}$ from the training data
using a structural support vector machine (SSVM)~\citep{joachims2009cutting}. 
The concept of SSVMs allows for a principled,
maximum-margin based, loss-sensitive training of CRFs. Learning the potential yields much
more complex interactions than the simple Potts potentials that are often used in the literature.

In general, structural SSVMs learn the parameters $\theta$ of a predictor of the form
\begin{equation}\eqlabel{prediction}
f(x) = \argmax_{y \in \mathcal{Y}} \theta^T \Phi(x, y).
\end{equation}
We choose $\psi$ in \Eqref{energy} to be linear in the learnable parameters and
the data-depended features, resulting in a form equivalent to
\Eqref{prediction}. Our features are described in detail below.  We use the
$1$-slack formulation of the structural SVM~\citep{joachims2009cutting} as
implemented in \pystruct and described in Chapter~\ref{ch:structured_pystruct}. The combination
of fusion moves and AD$^3$ described in Chapter~\ref{ch:exact_learning} allows us
to learn the SSVM to optimality \emph{exactly}.

\begin{figure}
    \begin{center}
        \includegraphics[width=\linewidth]{nyu/images/height_success}\\
        \vspace{3mm}
        \includegraphics[width=\linewidth]{nyu/images/height_failure}
    \end{center}
    \caption{%
        Visualization of the height computed using the method described in \Secref{unaries}.
        Input images are shown on the left (depth not shown), the computed height is depicted on the right.
        The top row exemplifies a typical scene, while the bottom row shows a scene without horizontal
        surfaces, where our method fails.
    \figlabel{height}}
\end{figure}

\subsection{Low Level Segmentation}
We take a super-pixel based approach to semantic segmentation.
Our superpixel generation is based on the SLIC
algorithm~\citep{achanta2012slic} described in \Secref{semantic_segmentation}. We
extend the standard SLIC algorithm, which works on the Lab space, to also
include depth information. The resulting algorithm is a localized $k$-Means in
Lab-D-XY space.
Our implementation is publicly available through the scikit-image library\footnote{\url{http://scikit-image.org}}.
Similar to \citet{SilbermanECCV12}, we found little visual improvement over the
RGB segmentation when using additional depth information. On the other hand,
estimation of per-superpixel features based on the 3D point cloud was more
robust when including depth information into the superpixel procedure.
%TODO visualize?
The resulting superpixels are compact in the 2D image. As the density of the
corresponding point cloud is dependent on the depth, we did not succeed in
creating superpixels that are compact in 3D while maintaining a meaningful
minimum size.

\subsection{Unary Image Features}\seclabel{unaries}
Our method builds on the probability output of a random forest, trained for pixel-wise
classification of the structure classes.
We use the GPU implementation provided by \citet{stueckler2013}\footnote{\url{https://github.com/deeplearningais/curfil}}.
The input for training are the full \mbox{RGB-D} images, transformed to Lab
color space. Each tree in the forest uses training pixels only from a subset of
training images. For each training image, an equal number of pixels for each
occurring class is sampled. Split features are given by difference of regions
on color or depth channels. Region size and offsets are normalized using the
depth at the target pixel.
We accumulate the probabilistic output for all pixels within a superpixel, and
use the resulting distribution as a feature for the unary node potentials in our CRF model.
We augment these prediction with another feature, based on the height of a superpixel in 3D.
This is a very informative feature, in particular to determine the floor.
To compute the height of a (super) pixel, we first find the ``up'' direction.
We use a very simple approach that we found effective: we cluster normal
directions of all pixels into 10 clusters using $k$-means, and use the one that
is most parallel to the $Y$ direction, which roughly corresponds to height in
the dataset.
We then project the 3D point cloud given by the depth along this direction, and
normalize the result between $0$ and $1$.
This procedure works robustly given there is some horizontal surface in the image,
such as the ground or a table. A few scenes contain only walls and furniture,
and the approach fails for these. \Figref{height} illustrates a typical case and
one of the much rarer failure cases.
While we could use a more elaborate scheme,
such as the one from \citet{SilbermanECCV12}, we suspect that the feature is of
little use in scenes without horizontal surfaces.
\begin{figure}
    \begin{center}
        \includegraphics[width=\linewidth]{nyu/images/normal_feature}
    \end{center}
    \caption{%
        Visualization of one of the pairwise features, the similarity between superpixel normals.
        The image shows the zoom-in of a bedroom scene, together with the superpixel over-segmentation.
        Lines connect adjacent superpixels, and line-strength gives the magnitude of the orientation similarity.
    \figlabel{normal_feature}}
\end{figure}

\subsection{Pairwise Depth-Sensitive Features}
There are five different features used to build pairwise potentials in our model:

\paragraph{Constant}
    A constant feature allows to model general neighborhood relations.


\paragraph{Color Contrast}
    We employ a non-linear color contrast, as is common in the computer
    vision literature, between the superpixel mean colors $c_i$ and $c_j$:
    \[
        \exp\left(-\gamma \lVert c_i - c_j \rVert^2\right).
     \]


\paragraph{Vertical Alignment}
    We model the directed angle between superpixel centers in the 2D image
    plane.  This allows the  model to learn that ``structure'' is above
    ``floor'', but not the other way around.


\paragraph{Depth Difference}
    We include the signed depth difference between superpixels, which
    allows the model to detect depth discontinuities that are not
    represented in the 2D neighborhood graph of the superpixels.


\paragraph{Normal Orientations}
    Differences in normal vector orientation are a strong clue on
    whether two superpixels belong to the same surface, and therefore the
    same structural class.
    We compute the 3D orientation of normals using the method of \citet{holz_2011_robocup},
    as implemented in the point cloud library (pcl)\footnote{\url{http://pointclouds.org}}.
    All normals within a superpixel are then averaged, to get a single orientation for each superpixel.
    The feature is computed as the difference of $\frac{\pi}{4}$ and the (undirected) angle between the normals belonging
    to two adjacent superpixels.  
    The feature is illustrated in \Figref{normal_feature}. The change
    in normal orientation highlights that pillow and wall are distinct
    objects, even though there is no strong distinction in color or depth.


\section{Experiments}\seclabel{experiments}

\begin{table*}[t]
\begin{tabularx}{\linewidth}{@{\extracolsep{\fill}}l@{}cccccc}
\toprule
                        & \footnotesize{ground}        &  \footnotesize{structure}    & \footnotesize{furniture}     & \footnotesize{props}         & \footnotesize{class avg.}   & \footnotesize{pixel avg.}\\
\cmidrule{1-7}

RF                              &         90.8  &   81.6        & 67.9          & 19.9          &  65.0        &  68.3 \\
RF + SP                         &         92.5  &   83.3        & \textbf{73.8} & 13.9          &  65.7        &  70.1 \\ 
RF + SP + SVM                   &         94.4  &   79.1        & 64.2          & \textbf{44.0} &  70.4        &  70.3 \\
RF + SP + CRF                   & \textbf{94.9} &   78.9        &          71.1 & 42.7          &\textbf{71.9} &  \textbf{72.3} \\
\cmidrule{1-7}
\citet{SilbermanECCV12}         &         68    &   59          & 70           & 42            &  59.6        & 58.6 \\
\citet{couprie-iclr-13}         &         87.3  & \textbf{86.1} & 45.3         & 35.5          &  63.5        & 64.5 \\
\cmidrule{1-7}
\citet{stueckler2013}$^\dagger$ & \textbf{95.6} &   83.0        & \textbf{75.1}& 14.2          &  67.0        & 70.9 \\

\bottomrule
\end{tabularx}
    \caption{Quantitative comparison of the proposed method with the
        literature.
The best value in each column is printed in bold$^\dagger$. The upper part of
the table shows contributions by different parts of our pipeline. RF stands for random forest prediction, RF + SP for aggregated
random forests prediction within superpixels, RF + SP + SVM for an SVM trained on the unary potentials, and RF + SP + CRF is
our proposed pipeline. We optimized our approach for class average\tablabel{results}
accuracy.\\
$^\dagger$ \footnotesize Note that the work of \citet{stueckler2013} is not directly
comparable, as they integrated information over multiple frames, and did not
measure accuracy for pixels without valid depth measurement.}
\end{table*}

We evaluate our approach on the public NYU depth V2 segmentation dataset of
indoor scenes.  The dataset comes with a detailed annotation of 1449 indoor
\mbox{RGB-D} images belonging to a wide variety of indoor scenes, categorized into 26
scene classes.  The annotation contains four semantic structural classes:
structure, floor, furniture and prop. As in the MSRC-21 and \textsc{Pascal} VOC
datasets, there is an additional ``void'' class, which is used for object
boundaries and hard-to-annotate regions. We follow the
literature in excluding these pixels completely from the evaluation.
We optimize our model for \emph{average class accuracy} (the mean of the
diagonal of the confusion matrix), which puts more emphasis on the harder
classes of props and furniture, which have smaller area than structure and
floor.

Our approach is implemented using our \textsc{PyStruct}
library introduced in \Secref{pystruct}.
%
All hyper-parameters were adjusted using 5-fold cross-validation. The
hyper parameters of the random forests were found using the \textsc{hyperopt}
framework of \citet{bergstra2011algorithms}. For the CRF model, the only
hyper-parameters are related to the superpixel segmentation, and the single
hyper-parameter $C$ of the structural SVM formulation. These were adjusted
using grid-search.
We found 500 superpixels per image to work best, which allow for a maximum
possible performance of 95\% average class accuracy on the validation set.

We observed that the linear programming relaxation
often found an integer solution, without the need for a branch-and-bound procedure. We also found
that using fusion moves alone produced inferior results.

\Tabref{results} compares different components of our approach with the literature.
Please note that we \emph{first} designed our final model, using only the
validation data. We now report accuracies of simpler models for reference,
however these results were not used for model selection. To separate
the influence of loss-based training and the spatial reasoning of the CRF,
we also train a usual support vector machine (SVM) on the unary potentials for comparison.

The random forest prediction, as reported in \citet{stueckler2013} is already quite competitive.
Grouping into superpixels slightly improves performance, by removing
high-frequency noise and snapping to object boundaries. Somewhat surprisingly,
using a standard unstructured SVM with rescaled loss already advances the mean
accuracy decidedly above the previous state-of-the-art. We
attribute this mostly to the ability of the SVM to exploit correlation between
classes and uncertainty within the superpixels.
Additionally, the SVM has access to the ``height'' feature, that was not
included in the random forest.
This performance is still improved upon, both in class average and pixel
average performance by the learned CRF approach, yielding
the best published result so far for both measures. The increase over the
standard SVM is $1.5\%$ for class average accuracy and $2.0\%$ for pixel
average accuracy.

A visualization of the impact of each processing step can be found in
\Figref{comparison}, which shows prediction results on the test set.  The four
prediction methods correspond to the rows of \Tabref{results}.  The difference
between the SVM and CRF approaches are clearly visible, with the CRF producing
results that are very close to the ground truth in several complex scenes.

We found that our approach improves results most for scenes with a clear geometric structure,
which is not surprising. We see that evidence from the random forest is often very noisy, and biased
away from the ``props'' class. While the unstructured SVM can correct somewhat
for the class imbalance, it has no way to make larger areas consistent, which
the CRF can.
On the other hand, performance of the CRF deteriorates slightly on very crowded
scenes with a mixture of small furniture and prop objects, as
can be seen in the two right-most images. In these scenes, depth information is
often noisy, and it is hard to make geometric statements on the superpixel
level. As the input from the random forest is also often of low quality for
crowded scenes, the CRF has little chance to recover.
%
\begin{figure}
    \begin{center}
        \includegraphics[width=.45\linewidth]{nyu/images/vertical_alignment}
    \includegraphics[width=.45\linewidth]{nyu/images/depth_difference}
    \end{center}
    \caption{%
        Visualization of some of the learned potentials. The right potential is
        applied to relative depth between superpixels, the second on the
        feature encoding whether one superpixel is above the other in the
        image. See \secref{experiments} for details.
    \figlabel{weights}}
\end{figure}
%
\Figref{weights} visualizes some learned potential functions.
Higher values correspond to favored configurations. One can see that the
vertical alignment potential expresses that the floor is much more likely to be below
other classes. It also encodes the fact that props
rest on furniture, but not the other way around.
The potential of the depth feature encodes, for example, that the ground
is usually behind the other classes, while furniture is in front of structures,
such as the wall.
Interestingly, the potential functions are not anti-symmetric, and forcing them to be so
degrades the results. This suggests that the direction of connecting edges, going from
the top left to the bottom right in the image, is also exploited by the potentials.


\begin{figure*}
    \begin{tabu} to \linewidth{@{}XXXXX@{}}
    \includegraphics[width=\linewidth]{nyu/images/00845_image.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00781_image.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01331_image.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00118_image.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01203_image.png}\\
    \multicolumn{5}{@{}c@{}}{Input Image}\vspace{3mm}\\


    \includegraphics[width=\linewidth]{nyu/images/00845_pixel.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00781_pixel.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01331_pixel.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00118_pixel.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01203_pixel.png}\\
    \multicolumn{5}{@{}c@{}}{Random Forest Prediction}\vspace{3mm}\\

    \includegraphics[width=\linewidth]{nyu/images/00845_sp.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00781_sp.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01331_sp.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00118_sp.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01203_sp.png}\\
    \multicolumn{5}{@{}c@{}}{Superpixel Voting}\vspace{3mm}\\

    \includegraphics[width=\linewidth]{nyu/images/00845_svm.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00781_svm.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01331_svm.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00118_svm.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01203_svm.png}\\
    \multicolumn{5}{@{}c@{}}{Support Vector Machine on Superpixels}\vspace{3mm}\\

    \includegraphics[width=\linewidth]{nyu/images/00845_ssvm.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00781_ssvm.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01331_ssvm.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00118_ssvm.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01203_ssvm.png}\\
    \multicolumn{5}{@{}c@{}}{Conditional Random Field on Superpixels}\vspace{3mm}\\

    \includegraphics[width=\linewidth]{nyu/images/00845_gt.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00781_gt.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01331_gt.png}&%
    \includegraphics[width=\linewidth]{nyu/images/00118_gt.png}&%
    \includegraphics[width=\linewidth]{nyu/images/01203_gt.png}\\
    \multicolumn{5}{@{}c@{}}{Ground Truth}\vspace{3mm}\\
    \multicolumn{5}{@{}c@{}}{%
        \includegraphics[]{nyu/legend.pdf}
    }
    \end{tabu}
\caption{%
Qualitative evaluation of the CRF\@.
The first three images illustrate errors in the original prediction that can be corrected, while
the second two images illustrate failure modes. Pixels marked as void
are excluded from the evaluation. See \Secref{experiments} for details.
\figlabel{comparison}}
\end{figure*}

\section{Summary and Discussion}
We introduce a CRF formulation for semantic segmentation
of structure classes in \mbox{RGB-D} images. We base our model in the output of an efficient GPU implementation
of random forest, and model spatial neighborhood using a superpixel-based approach.
We combine color, depth and 3D orientation features into an energy function
that is learned using the SSVM approach. By explicitly modeling 3D relations in
a fully learned framework, we improve the state-of-the-art on the NYU V2
dataset for semantic annotation of structure classes.


While our approach allows modeling of spatial relations, these are limited to
local interactions. In future work, these interactions could be extended to
larger areas using latent variable models or higher
order potentials~\citep{ladicky2009associative}.
Another possible line of future investigation is to combine our approach with a more
task-specific one, directly including support plane assumptions into the model,
as done by \citet{SilbermanECCV12}. Finally, we could also combine our
single-frame approach with the approach of \citet{stueckler2013}, which fuses
individual views in 3D to exploit temporal coherence.

We did not explicitly address real time application; the random
forest implementation that we build upon allows for real-time
processing~\citep{stueckler2013}. The SLIC superpixel algorithm can also be
implemented on GPU in real-time, as was demonstrated by \citep{ren2011gslic}, and similarly
the normal features we use also have
real-time capabilities~\citep{holz_2011_robocup}.
Finally, fusion move inference for our model is very efficient for our model,
opening up the possibility to implement our approach entirely in real time.
