
\section{Appendix}
\begin{lemma}\label{binarylemma}
Given two (dependent) binary random variables $x,y$:
\begin{equation}
p(x \neq y) = p(x=0) + p(y=0) - 2 p(x=0, y=0)
\end{equation}

\begin{proof}
\begin{align}
p(x \neq y) & = p(x=0, y=1) + p(x=1, y=0) \\
                &= p(x=0) - p(x=0, y=0)\\
                &  + p(y=0) - p(x=0, y=0) \\
                &= p(x=0) + p(y=0) -2 p(x=0, y=0)
\end{align}
\end{proof}
\end{lemma}

\begin{theorema}
Let all instances be independent. With bags $X$, instances $x$, true labels $y,Y$, $h_I$ any classifier
on instances and $h_B$ the resulting bag classifier, we have
\begin{align*}
   e_I = h_0 + p(y=0) - 2 \left (\frac{1}{2} ( h_0^r + p(y=0)^r - e_B) \right)^\frac{1}{r}
\end{align*}
\begin{proof}
    
\begin{align*}
e_B & = p(h_B(X) = 0) + p(Y=0) -2 p(h_B(X)=0, Y=0) \\\
&\text{using the lemma}\\
& = h_0^r + p(y=0)^r -2 p(h_I(x)=0, y=0)^r\\\
&\text{by independence}\\
& = h_0^r + p(y=0)^r -2 \left ( \frac{1}{2} (h_0 + p(y=0) - e_I) \right ) ^r \\
&\text{using Lemma~\ref{binarylemma} on }p(h_I(x)=0, y=0.
\end{align*}
Solving for $p(h_I(x)\neq y)$:
\begin{align*}
    & \left ( \frac{1}{2} (h_0 + p(y=0) - e_I )\right ) ^r = \frac{1}{2} \left ( h_0^r + p(y=0)^r - e_B \right )\\
    & \Rightarrow \frac{1}{2} (h_0 + p(y=0) - e_I)   = \left ( \frac{1}{2} \left ( h_0^r + p(y=0)^r - e_B \right ) \right )^\frac{1}{r}\\
    & \Rightarrow e_I   = h_0 + p(y=0) - 2 \left ( \frac{1}{2} \left ( h_0^r + p(y=0)^r - e_B \right ) \right )^\frac{1}{r}
\end{align*}
\end{proof}
\end{theorema}

\begin{corollarya}
If all instances are independent and bag classification is perfect, meaning $e_B=0$, then instance classification is also perfect ($e_I =0$).

\begin{proof}
Consider Theorem~\ref{basicthm}. If $e_B=0$ we have
\begin{align*}
p(h_I(x) \neq y) = h_0 + p(y=0) - 2 \left (\frac{1}{2} ( h_0^r + p(y=0)^r)) \right)^\frac{1}{r}
\end{align*}
By the convexity of the power function the right hand side of the equation is always greater or equal zero.
Since $e_I \geq 0$, both sides are equal zero.
\end{proof}
\end{corollarya}

\begin{theorema}
    If $h_0^r  \geq e_B$ and all instances are independent, we have
\begin{align}
    p(h_I(x)\neq y) \leq h_0 - \left ( h_0^r - e_B) \right ) ^ \frac{1}{r}
\end{align}
\begin{proof}
    Assume $h_0^r  \geq e_B$.
    From Theorem~\ref{basicthm} we have
\begin{align*}
   e_I &= h_0 + p(y=0) - 2 \left (\frac{1}{2} ( h_0^r + p(y=0)^r - e_B) \right)^\frac{1}{r}\\
   &\leq h_0 + p(y=0) - \left [( h_0^r + p(y=0)^r - e_B) \right]^\frac{1}{r}\\
   &- \left ( p(y=0)^r \right )^\frac{1}{r}\\
   &= h_0 - \left (( h_0^r + p(y=0)^r - e_B) \right)^\frac{1}{r}\\
\end{align*}
\end{proof}
\end{theorema}

\begin{theorema}
Let all instances be independent. Given $p(Y)$, the instance error $e_I$ can be calculated by
\begin{align}
    %\eqlabel{iid}
e_I = h_0 + p(Y=0)^\frac{1}{r} - 2 \left (\frac{1}{2} ( h_0^r + p(Y=0) - e_B) \right)^\frac{1}{r}.
\end{align}
\begin{proof}
    This follows from Theorem~\ref{basicthm} by using $p(Y=0)=p(y=0)^r$.
\end{proof}
\end{theorema}


\begin{theorema}
If the instance labels are independent inside a bag given a hidden cause, the instance
error $e_I$ has the following lower bound:
\begin{multline*}
e_I \geq h_0 + p(y=0) - \\
2 \left (\frac{1}{2} (p(h_B(X)=0) + p(Y=0) - e_B) \right)^\frac{1}{r}
\end{multline*}
\begin{proof}
    This follows from taking the expected value with respect to $Z$ of \eqref{conditioned} and using Jensen's
    inequality.
\end{proof}
\end{theorema}
