\documentclass[12pt,toc=bibnumbered, a4paper,twoside,DIV=11,BCOR=1cm]{scrbook}
%\documentclass[12pt,toc=bibnumbered, a4paper,twoside,DIV=calc]{scrbook}
%\documentclass[12pt,toc=bibnumbered, a4paper,twoside]{book}

\usepackage[colorlinks=True, citecolor=blue!50!black, linkcolor=red!50!black, urlcolor=blue!70!black]{hyperref}
\usepackage{refstyle}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{tabu}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{mdwlist}
\usepackage[noend]{algpseudocode}
\usepackage[small]{caption}
\usepackage{setspace}
\usepackage{color}
\usepackage{minted}
\usepackage[german,english]{babel}
\usepackage[export]{adjustbox}

%\renewcommand*\sfdefault{avant}
\usepackage{helvet}
%\usepackage[defaultsans]{droidsans}
%\renewcommand*\sfdefault{lmssq}

\usepackage{garamondx}
%\usepackage[urw-garamond]{mathdesign}
\usepackage{xspace}
\onehalfspacing
\KOMAoptions{DIV=last}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\x}{\checkmark}
\renewcommand{\o}{$\times$}

\makeatletter
\let\OldStatex\Statex
\renewcommand{\Statex}[1][3]{%
\setlength\@tempdima{\algorithmicindent}%
\OldStatex\hskip\dimexpr#1\@tempdima\relax}
\makeatother

\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\newminted{python}{frame=single,rulecolor=\color{rulecolor}, linenos, xleftmargin=7mm}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newcommand{\pystruct}{\textsc{PyStruct}\xspace}
\newcommand{\svmstruct}{SVM$^\text{struct}$\xspace}

\algrenewcommand{\algorithmicrequire}{\textbf{Input:}}
\algrenewcommand{\algorithmicensure}{\textbf{Output:}}

\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
\newcolumntype{C}{>{\centered\arraybackslash}X}

\newcommand{\fix}{\marginpar{FIX}} \newcommand{\new}{\marginpar{NEW}}
\newcommand\etal{~et\,al.~}

\newcommand\hzero{\ensuremath{p(h_I(x)=0)}}
\newcommand\bagerror{\ensuremath{p(h_B(X) \neq Y)}}
\newcommand\instanceerror{\ensuremath{p(h_I(x) \neq y)}}

%\newcommand{\hoch}[1]{^{(#1)}}
\newcommand{\hoch}[1]{^{#1}}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\W}{\mathcal{W}}


\newcommand{\given}{\, | \,}
\newcommand{\B}[1]{\mathbf{#1}}

\setlength{\tabcolsep}{2pt}

%\newcolumntype Y{S[group-four-digits=true, per-mode=symbol, round-precision=0, table-number-alignment=right]}
\newcolumntype Y{X[c,1]{S[group-four-digits=true, table-format=2.2, table-number-alignment=right]}}
\tabucolumn Y


\begin{document}
%
\begin{spacing}{1.5}
\begin{titlepage}
\vspace*{1mm}
\begin{center}
\Large\sffamily
\textbf{{\huge Methods for Learning Structured Prediction in Semantic Segmentation of Natural Images}}\\[5mm]
Dissertation\\
zur\\
Erlangung des Doktorgrades (Dr.\ rer.\ nat.)\\
der\\
Mathematisch-Naturwissenschaftlichen Fakult\"at\\
der\\
Rheinischen Friedrich-Wilhelm Universit\"at Bonn\\
vorgelegt von\\
{\LARGE Andreas Christian M\"uller}\\
aus\\
Offenbach am Main\\
Bonn, September 2013\\[10mm]

\end{center}
\hspace{3.4cm}\includegraphics[height=2.0cm]{ais_uni_logo}
\end{titlepage}
%
%\thispagestyle{empty}
\clearpage
\pagenumbering{Roman}
\setcounter{page}{2}
\vspace*{5cm}
\Large\sffamily
\begin{center}
Angefertigt mit Genehmigung\\
der Mathematisch-Naturwissenschaftlichen Fakult\"at\\
der Rheinischen Friedrich-Wilhelms-Universit\"at Bonn\\
\end{center}
1. Gutachter Prof. Dr. Sven Behnke\\
2. Gutachter Prof. Dr. Reinhard Klein\\
Tag der Promotion:\\
Erscheinungsjahr:\\
\end{spacing}
\tableofcontents
%\listoffigures
%\listoftables
\chapter*{Zusammenfassung}
\enlargethispage{10mm}
\selectlanguage{german}
Automatische Segmentierung und Erkennung von semantischen Klassen in
nat\"urlichen Bildern ist ein wichtiges offenes Problem des maschinellen Sehens.
In dieser Arbeit untersuchen wir drei m\"oglichen Ans\"atze der Erkennung:
ohne \"Uberwachung, mit \"Uberwachung auf Ebene von Bildern und mit \"Uberwachung auf Ebene
von Pixeln.

Diese Arbeit setzt sich aus drei Teilen zusammen.
Im ersten Teil der Arbeit schlagen wir einen Clustering-Algorithmus vor,
der eine neuartige, informationstheoretische Zielfunktion optimiert. Wir zeigen, dass
der vorgestellte Algorithmus \"ublichen Standardverfahren aus der Literatur gegen\"uber
klare Vorteile auf vielen verschiedenen Datens\"atzen hat. Clustering ist ein wichtiger
Baustein in vielen Applikationen des machinellen Sehens, insbesondere in der
automatischen Segmentierung.

Der zweite Teil dieser Arbeit stellt ein Verfahren zur automatischen
Segmentierung und Erkennung von Objektklassen in nat\"urlichen Bildern vor, das
mit Hilfe von Supervision in Form von Klassen-Vorkommen auf Bildern in der Lage
ist ein Segmentierungsmodell zu lernen.

Der dritte Teil der Arbeit untersucht einen der am weitesten
verbreiteten Ans\"atze zur semantischen Segmentierung und
Objektklassensegmentierung, Conditional Random Fields, verbunden mit Verfahren
der strukturierten Vorhersage.
%Die Vorhersage f\"ur Modelle zur Bild-Markierung
%mittels Conditional Random Fields ist f\"ur gew\"ohnlich nur approiximativ
%moeglich, bedingt d\"urch das Vorhandensein von Kreisen in den zugrunde
%liegenden Nachbarschaftsgraphen.  Aus diesem Grund, und trotz der Beliebtheit
%von Conditional Random Field Verfahren, ist das strukturierte Lernen f\"ur
%solche Anwendungen schlecht verstanden.
%
Wir untersuchen verschiedene Lernalgorithmen des strukturierten Lernens,
insbesondere im Zusammenhang mit approximativer Vorhersage. Wir zeigen, dass es
m\"oglich ist trotz des Vorhandenseins von Kreisen in den betrachteten Nachbarschaftsgraphen
exakte strukturierte Modelle zur Bildsegmentierung zu lernen.  Mit den
vorgestellten Methoden bringen wir den Stand der Kunst auf zwei komplexen
Datens\"atzen zur semantischen Segmentierung voran, dem MSRC-21 Datensatz von
RGB-Bildern und dem NYU V2 Datensatz von RGB-D Bildern von Innenraum-Szenen.

\enlargethispage{15mm}
Wir stellen au{\ss}erdem eine Software-Bibliothek vor, die es erlaubt einen
weitreichenden Vergleich der besten Lernverfahren f\"ur strukturiertes Lernen
durchzuf\"uhren.
Unsere Studie erlaubt uns eine Charakterisierung der betrachteten Algorithmen
in einer Reihe von Anwendungen, insbesondere der semantischen Segmentierung und
Objektklassensegmentierung.

\selectlanguage{english}
\chapter*{Abstract}
Automatic segmentation and recognition of semantic classes in natural images is
an important open problem in computer vision. In this work, we investigate
three different approaches to recognition: without supervision, with
supervision on level of images, and with supervision on the level of pixels.
The thesis comprises three parts.

The first part introduces a clustering algorithm that optimizes a novel
information-theoretic objective function. We show that the proposed algorithm
has clear advantages over standard algorithms from the literature on a wide
array of datasets. Clustering algorithms are an important building block for
higher-level computer vision applications, in particular for semantic
segmentation.

The second part of this work proposes an algorithm for automatic segmentation
and recognition of object classes in natural images, that learns a segmentation
model solely from annotation in the form of presence and absence of object classes
in images.

The third and main part of this work investigates one of the most popular
approaches to the task of object class segmentation and semantic segmentation,
based on conditional random fields and structured prediction.
%
We investigate several learning algorithms, in particular in combination with
approximate inference procedures. We show how structured models for image
segmentation can be learned exactly in practical settings, even in the presence
of many loops in the underlying neighborhood graphs.
The introduced methods provide results advancing the state-of-the-art on two
complex benchmark datasets for semantic segmentation, the MSRC-21 Dataset of RGB
images and the NYU V2 Dataset or RGB-D images of indoor scenes.
%
Finally, we introduce a software library that allows us to perform extensive empirical
comparisons of state-of-the-art structured learning approaches. This allows us
to characterize their practical properties in a range of applications, in
particular for semantic segmentation and object class segmentation.


\chapter*{Acknowledgements}
First, I would like to thank my advisor Sven Behnke, who allowed me
to pursue my PhD in his department and who provided funding for my studies.
I would also like to thank J\"urgen Gall for agreeing to be my second reader.

During my work on this dissertation, my ideas were shaped by many of my fellow
students and researchers. First and foremost I am in debt to Hannes Schulz,
whose feedback proved to be invaluable during my studies.
I would like to thank Christoph Lampert for hosting me as a visiting researcher
at the IST Austria. His guidance and advice helped me in directing my further
research. My thanks are also extended to Sebastian Nowozin for his collaboration,
many helpful discussions, and advice.

I would also like to thank Carsten Rother for allowing me to work with him
at Microsoft Research Cambridge and hosting me there.

I have been lucky to be a part of the \textsc{scikit-learn} community, whose developers
have been a constant source of encouragement and inspiration to me. My
understanding of machine learning as well as software engineering has been
greatly affected by the continuing exchange with Olivier Grisel, Lars Buitinck,
Ga\"el Varoquax, Vlad Niculae, Mathieu Blondel, Gilles Louppe, Arnaud Joly and
others.

I also thank my parents and my sister for their support.

Last but not least I am grateful to Anna M\"uller for her support,
encouragement and company. Thank you for giving me the strength needed for this
endeavour.
\cleardoublepage
\pagenumbering{arabic}
\include{intro}

%\include{datasets/paper}

\include{itm/paper}

\include{partial_supervision/paper}

\include{structured_prediction/paper}
\include{pystruct/paper}

\include{evaluation/paper}

\include{exact_learning/paper}

\include{nyu/paper}

\chapter{Conclusion}
%\section{Summary}
In this thesis, we explored the use of structured prediction methods for semantic
segmentation and object class segmentation of natural images, an important
step towards general scene understanding.
We use the paradigm of structured prediction, which allows for a principled
integration of context and object relations.
%Current research in computer vision progresses at an incredible speed, with new
%methods and applications emerging constantly. Still, there are large
%differences in the paradigms employed by different researchers. While it is
%generally acknowledged that context is one of the most important cues in
%recognizing objects and interpreting scenes, there is little consensus on how
%contextual information should be modelled, or even learned.
%The structured prediction paradigm allows a principled approach to
%incorporating predictions within an image, and to include context information.
%We investigated the potential of structural methods for the application of
%semantic image segmentation and object recognition, applications to which
%structured methods have been applied successfully before.
We focused on \emph{learning} of structural models and the interaction of
inference and learning in the neighborhood models typically employed for
semantic segmentation.
We presented an open source software implementation of a variety of
popular learning algorithms for structural support vector machines, together
with a thorough evaluation of their properties, in particulare when using approximate
inference. Our software provides a foundation for future research into learning,
inference and models for computer vision by providing extensive examples and
benchmarks.

We showed that effective use of available inference mechanisms enables
exact learning, even in the presence of loops in the underlying
factor graph. Our methods achieve competitive performance with similar
 methods on the Pascal VOC 2010 dataset, and improve upon state-of-the-art
 results on the MSRC-21 dataset.
We demonstrated the power of conditional interactions by learning spatial
interactions in an RGB-D setting. Here, our approach improves upon the state-of-the
art on the NYU V2 benchmark for annotation of semantic structure classes.

We also presented a novel approach for clustering based on information
theoretic principles. Our algorithm improves upon methods from the literature
in finding pre-defined classes on a wide range of datasets. This indicates that
in the task of extracting superpixels, we can also hope to achieve better
results than the $k$-means based SLIC algorithm that we used.
\enlargethispage{1cm}
As manual annotation of images for learning semantic segmentation and object
class recognition is laborious and error-prone, we suggested a method to
learn object class segmentation for complex object classes from image-level
annotations alone. Our approach is formulated using multiple instance learning
over a set of candidate segments. We demonstrated the feasibility and effectiveness
of our approach on the challenging Graz-02 dataset of street scenes.

\section{Future Directions}
There are several directions for future research that we think would be
interesting to pursue as an extension of the presented results:
%
\paragraph{Large-Scale Weakly Supervised Object Class Segmentation}
We demonstrated a new method for object class segmentation using only weak
supervision. One of the main advantages of such a method is that it is
potentially able to exploit the large amount of weakly labeled data that
is available on the internet. Using additional, weakly labeled training data,
and evaluating on the given, manually annotated data, is therefore a promising
path for improving the presented results.
%
\paragraph{Cached Inference for SZLJSP}
We saw in Chapter~\ref{ch:comparison} that the $1$-slack cutting plane algorithm
benefits immensely from caching inference results during training. Therefore, investigating
the influence of caching for SZLJSP seems a promising topic for future research.
%
\paragraph{Theoretical Analysis of $n$-slack Algorithm}
As we have seen in Chapter~\ref{ch:comparison}, the $n$-slack algorithm often
converges very fast in terms of passes over the training data This is in stark
contrast to the known theoretical convergence guarantee, which is the slowest
of all the algorithms we considered with
$O(\frac{1}{\epsilon^2})$\footnote{This is in terms of calls to the QP\@. We are
not aware of any analysis in terms of inference calls or passes over the
training set.}. It seem as if the approach of \citet{lacoste2012block} can
yield a better convergence guarantee, but it is also worth investigating the
direction pursued by \citet{shalev2012proximal}.
%
\paragraph{Inference Machines}
Recently \citet{stoyanov2011empirical} started a new trend in structured
prediction, which is sometimes called ``inference machines''. The basic
principle is simple: the process of prediction \emph{using a given inference
procedure} is viewed as a feed-forward method for prediction, and parameters of this
prediction process are optimized directly using empirical risk minimization. The work of
\citet{stoyanov2011empirical} used loopy belief propagation as their inference
algorithm and the optimization is carried out simply using gradient descent on
the non-convex but differentiable loss function. Other recent work in this
direction includes \citet{krahenbuhlparameter}, who used mean-field inference
in a fully connected conditional random field and \citet{jancsarylearning}, who
used closed form inference in a Gaussian CRF\@.
While these algorithms show great promise, their relation to the traditional
approach of structured prediction used in this work is mostly unclear. In particular,
if exact traditional learning is possible in a model, it is uncertain how much
in accuracy and efficiency can be gained by direct empirical risk minimization.
Only limited empirical comparison is available, and we are not aware of theoretical
work in this direction, leaving much room for future investigation.
%
\paragraph{Non-Linear Models}
In this work, we only considered models that are linear in the input features---though
features are highly non-linear in the original input pixels. Allowing non-linear
interactions increases the representational power of a CRF, possibly leading
to more accurate prediction results.
Kernelization of structural support vector machines is straight-forward in theory, but
had only limited success in the context of CRFs for image
segmentation~\citep{lucchi2012structured}.
Two major alternatives for non-linear CRFs were proposed in the literature,
conditional neural fields~\citep{peng2009conditional} based on neural networks,
and decision tree fields~\citep{nowozin2011decision}, based on decision trees.
Conditional neural fields have only been applied to sequence classification so far,
and extending them to our setting of semantic image segmentation would be very
interesting. Decision tree fields~(DTFs) on the other hand have been applied to loopy
graphs for image processing, but not for higher-level tasks such as semantic
segmentation. If it is possible to include context in a meaningful way, it
might be possible to address even object-centric tasks such as object class
segmentation with DTFs.
%
\paragraph{Higher Order Potentials and Latent Variable Models}
While non-linear potentials would allow for more complex interactions between
inputs and labelings, introducing higher order potentials or latent variables
allow the model to express more complex interactions within the output
variables~\citep{dann2012pottics, kohli2009robust}. Possible examples are
consistency of larger regions, learning parts or learning of scene classes and
cooccurences.
In principle, higher order potentials and latent variable models are
equivalent, in that each energy function expressed in either form can be
transformed into an energy function of the other kind. In practice, learning of
higher order potentials for semantic segmentation has received little
attention, while approaches using latent variables are often limited by
the non-convexity of learning.  It would be interesting to compare current
methods using latent variable and higher order approaches, and see how these
interact with different inference and learning schemes.
%
\paragraph{Feature Design}
This work mostly focused on learning methods, and less on the input---with
the exception of Chapter~\ref{ch:nyu}, which explores the use of 3D features
for semantic segmentation of indoor scenes. It is clear, however, that
the input features play an important role in the performance of any system.
Using our approach for exact learning of loopy graphs, it seems to be worthwhile to
revisit the works of \citet{nowozin2010parameter} and \citet{lucchi2011spatial},
that evaluate the impact of input features and piecewise training, and of the importance
of global constraints versus global features, respectively.
In particular the importance of features for pairwise potentials has been
somewhat overlooked in the computer vision literature, often being reduced to a
single constant or contrast sensitive feature.

\bibliographystyle{plainnat}
\bibliography{main}
\end{document}
